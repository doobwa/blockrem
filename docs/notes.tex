\pagestyle{empty}
\documentclass[11pt]{article}
\usepackage{tikz}
\tikzstyle{vertex}=[auto=left,circle,fill=black!25,minimum size=20pt,inner sep=0pt]

%\usepackage{tikz,fullpage}
%\usetikzlibrary{arrows,petri,topaths}%
%\usepackage{tkz-berge}
\usepackage{verbatim}
\usepackage{url}
\usepackage{graphicx, color}
\usepackage{amsmath}
\usepackage{subfigure}
%\usepackage[small,compact]{titlesec} 
%\usepackage[small,it]{caption}

\textwidth 6.5in
\headheight .1in
\topmargin -.2 in
\textheight 8.7in
\oddsidemargin 0in
\evensidemargin 1in

\begin{document} 

 \centerline{\Large \bf Stochastic Blockmodels for Relational Event Data} 
 \medskip
\centerline{\bf Christopher DuBois}
 \bigskip

\abstract{
Observing network data as a stream of events provides an opportunity to understand differences in how nodes behave given the past.  Recent methods for continuous-time network data model the rate of dyadic events conditioned on the observed history of events and covariates about the actors.  Methods for static networks, such as stochastic blockmodels, often account for actor-level heterogeneity by assuming latent groups of individuals have similar tendencies in their group-wise interactions.  We propose to combine these two approaches by modeling the event dynamics within and between clusters of nodes.  The method is illustrated using dyadic interaction data -- e.g. email, Twitter direct messages.  We show parameter estimates from the model reveal heterogeneity in the dynamics among groups of individuals.  The fitted models have better predictive accuracy than both baselines and relational event models without the latent structure.  Our approach illustrates the efficacy of combining a detailed model for local dependencies and a latent variable model for meso-scale dependencies.

%Stochastic blockmodels are a tool for exploring the structure of static social networks by learning latent groups of individuals who have similar tendencies in their  group-wise interactions. Continuous-time methods, such as the relational event framework (Butts 2008), model the rate of dyadic events conditioned on the observed history of events and covariates about the actors.  We propose a hierarchical model combining these two approaches where each pair of blocks has a set of parameters guiding the event dynamics for dyads between those blocks.  The  method is illustrated using  dyadic interaction data (e.g. email, Twitter) among hundreds of actors and thousands of events.  We show parameter estimates from the model can reveal heterogeneity in the dynamics among groups of individuals, providing a better fit to the data than a single relational event model for all actors in the network.  Our approach illustrates the efficacy of combining a detailed model for local dependencies and a latent variable model for meso-scale dependencies.

% \begin{itemize}
% \item Blockmodels and clustering algorithms often applied to static networks for exploratory data analysis
% \item continuous time methods allow one to predict future events given the previous history
% \item we combine these two ideas with a hierarchical model such that each pair of blocks has a set of parameters guiding the event dynamics for those dyads
% \item prediction experiments on real datasets
% \item show that the parameter estimates are interpretable
% \end{itemize}

}

\section{Introduction}

Statistical methods for analyzing network data have become increasingly useful for studying  phenomenon ranging from people communicating online to protein interactions \cite{Goldenberg2009}.  One class of statistical models for network data uses latent variables to help model unobserved heterogeneity.  For example, the stochastic blockmodel \cite{Nowicki2001, Kemp} assumes each node in the network belongs to some block (or cluster) and parametrizes the probability of edges between nodes of each block.  % Such approaches are especially useful for large-scale network data where higher-order dependencies can cause models such as exponential random graph models to be too complex to fit.

Network data, however, is often collected as a sequence of events occurring over time.   Recently work leverages  continuous time models from event history analysis  \cite{AalenOddO.2008} to model network-based event data \cite{Butts2008,Brandes2009,Perry2011,Stadtfeld2010,Stadtfeld2011,Opsahl2011,Vu2011,Vu2011a}.  These models allow one to specify how the process depends on the previous history of events.  In this way one may investigate theories about the underlying processes and make predictions about future data conditioned on the past.

The above continuous time network models assume all nodes in the network behave according to identical dynamics.  In many cases this is unrealistic.  For example, in a small organization comprised of several teams, each team may exhibit different patterns for collaborating via email --  individuals in one group may respond more quickly, while another group may preferentially send to highly active individuals.  %, and in many cases emails between one pair of individuals will not affect emailing behavior between a separate pair of individuals.

Borrowing from the intuition of stochastic blockmodels, we propose a hierarchical model of continuous time network data that assumes latent clusters of nodes share similar patterns of interaction.  The behavior of the model is illustrated with simulated data.  We describe parameter estimation and the learning of the latent cluster assignments via MCMC.  Using data sets of dyadic communication among people, we compare the predictive performance of the fitted models to standard baselines.  Finally we show that the parameter estimates provide an interpretable structure to the event dynamics, sometimes identifying particular \emph{roles} of communication that exist.
  
\section{Model}

%Each interaction then has a set of periods $d \in D_{ij}$ each with a start time $a_d$, an end time $b_d$, and a rate $\lambda_d$.
Consider a nonhomogeneous Poisson process with  intensity $\lambda(t)$ that is piecewise constant with respect to a set of knots $\tau$.  We can then write the likelihood of $M$ events as
\begin{align}
\mathcal{L}(\mathcal{A}|\theta) &= \prod_{m=1}^M \lambda(t_m) \exp\left\{ - \int_{0}^{t_M} \lambda(s)ds \right\} \\
&= \prod_{m=1}^M \lambda(t_m) \prod_{k=1}^{|\tau|} \exp\left\{ - (\tau_{k} - \tau_{k-1}) \lambda(\tau_k) \right\}
\end{align}
\noindent where the $m$th event occurs at time $t_m$, the intensity function $\lambda(t)$ is left continuous, and each $\tau_k \in [0,t_M]$.
% \begin{align}
% \mathcal{L}(A|\theta) &= \prod_{m=1}^M \lambda_{i_m}(t_m|\cdot) \prod_{i \in \mathcal{R}}  \exp\left\{ - \int_{0}^{t_M} \lambda_{i}(\tau | \cdot)d\tau \right\} \\
% &= \prod_{m=1}^M \lambda_{i_m}(t_m|\cdot) \prod_{i \in \mathcal{R}} \prod_{v \in D_{i}} \exp\left\{ - (b_v - a_v) \lambda_{i}(b_v | \cdot ) \right\}
% \end{align}

One can extend the above to marked point processes where each event contains additional information.  In the context of relational events occurring among $N$ nodes, each event in the process contains both a sender $i$ and a recipient $j$ such that  $(i,j) \in \mathcal{R}$, where $\mathcal{R}$ is the \emph{risk set} comprised of all allowed dyads.  We assume each dyad $(i,j) \in \mathcal{R}$ has an intensity function $\lambda_{ij}(t)$ that is piecewise constant.  We can then write the likelihood as  
\begin{align}
\mathcal{L}(\mathcal{A}|\theta) &= \prod_{m=1}^M \lambda_{i_m,j_m}(t_m|\cdot) \prod_{(i,j) \in \mathcal{R}_{i_m,j_m}}\exp\{ - (t_m - \tau_{mij}) \lambda_{ij}(t_m | z_i,z_j) \}
\label{eqn:llk}
\end{align}
\noindent where event $m$ is the dyad $(i_m,j_m)$, $\tau_{i,j,m}$ is the time  of the changepoint for $\lambda_{i,j}$ prior to the $m$th event, and $\mathcal{R}_{i,j}$ is the set of dyads whose intensity changes if $(i,j)$ occurs. \footnote{We further assume 1) all intensity functions change at times $t=0$ and $t=t_M$, and 2) the first event is drawn uniformly from the risk set.}

\begin{figure}

\centering 
\subfigure[Dynamic network data]
{
\begin{tikzpicture}[scale=.8,transform shape,thick]
  \node[vertex] (n1) at (0,0)  {1};
  \node[vertex] (n2) at (2,2)  {2};
  \node[vertex] (n3) at (4,0) {3};
  \node[vertex] (n4) at (2,-2) {4};
\draw[-stealth] (.4,.4) -- (1.6,1.6);
\draw[-stealth] (2.4,1.6) -- (3.6,.4);
\node at (.7,1.1) {$t_1$};
\node at (3.3,1.1) {$t_2$};
\end{tikzpicture}
}
\subfigure[Intensities for two dyads]
{
\begin{tikzpicture}[scale=1,transform shape,thick]
\draw[-stealth] (0,0) -- (0,3);
\draw[-stealth] (0,0) -- (5,0);

% First timeline
\draw (0,2) -- (1,2);
\draw[fill=black] (1,2) circle (0.75mm);
\draw (1,1.7) circle (0.75mm);
\draw (1.08,1.7) -- (3,1.7);
\draw[fill=black] (3,1.7) circle (0.75mm);
\draw (3,1.7) circle (0.75mm);
\draw (3,2.2) circle (0.75mm);
\draw[-stealth] (3.05,2.2) -- (5,2.2);

% second timeline
\draw[dashed] (0,1) -- (3,1);
\draw[fill=black] (3,1) circle (0.75mm);
\draw (3,.75) circle (0.75mm);
\draw[dashed, -stealth] (3.08,.75) -- (5,.75);

% labels
\node at (-.5,2) {$\lambda_{1,2}$};
\node at (-.5,1) {$\lambda_{3,4}$};
\node at (1,-.3) {$t_1$};
\node at (3,-.3) {$t_2$};
\end{tikzpicture}
}
\caption[]{Illustration of event data and the assumptions of the model.  Left: An sequence of two events among four nodes: (1,2) occurs at time $t_1$ and (2,3) occurs at time $t_2$.  Right: The intensity functions $\lambda_{1,2}(t)$ (solid) and $\lambda_{34}(t)$ (dotted).  Note $\lambda_{34}$ can only change when events occur involving either node 3 or node 4.\footnotemark }
\label{fig:example}
\end{figure}

\footnotetext{TODO? Illustration of the block model, where events in cluster 1 (circles) are governed by the vector $\beta_{1,1}$, events in cluster 2 (squares) are governed by $\beta_{2,2}$, and an circle-to-square event governed by $\beta_{1,2}$.}


% \begin{figure}
%  \def\svgwidth{6in}
%   \input{example1.pdf_tex}
% \caption{Illustration of event data and the assumptions of the model.  Left: An sequence of two events among four nodes: (1,2) occurs at time $t_1$ and (2,3) occurs at time $t_2$.  Center: The intensity functions $\lambda_{1,2}(t)$ (solid) and $\lambda_{34}(t)$ (dotted).  Note $\lambda_{34}$ can only change when events occur involving either node 3 or node 4.  Right: Illustration of the block model, where events in cluster 1 (circles) are governed by the vector $\beta_{1,1}$, events in cluster 2 (squares) are governed by $\beta_{2,2}$, and an circle-to-square event governed by $\beta_{1,2}$.}
% \label{fig:example}
% %\includegraphics[width=6in]{example}
% \end{figure}

For a given dyad $(i,j)$ and time $t$ we may also have a vector of statistics $\mathbf{s}(t,i,j|\mathcal{A}_t,\mathbf{z})$ that is a function of the previous event history  $\mathcal{A}_t = \{(t_m,i_m,j_m): t_m \in [0,t)\}$.   Though this will be useful, we will constrain the types of dependencies so that an interaction between one dyad does not affect the rate of an entirely separate dyad.  As shown in Figure \ref{fig:example}, we do this by assuming each intensity function $\lambda_{ij}(t)$ may only change following an event involving either $i$ or $j$.  This implies that  $\mathcal{R}_{i,j}$ is the set of dyads involving either $i$ or $j$ and that we will only use  statistics $\mathbf{s}(t,i,j)$ that do not change until an event involving either $i$ or $j$ occurs.  

We hope to learn about the dynamics between subsets of nodes while allowing for variety among disjoint subsets.   To facilitate this, we assume node $i$ is assigned a latent cluster $z_i$ and the intensity functions via a log linear model
\begin{align*}
\log \lambda_{ij}(t | \mathcal{A}_t,\mathbf{z}) &= \boldsymbol{\beta}_{z_i,z_j} \mathbf{s}(t,i,j|\mathcal{A}_t) &
z_i &\sim \mbox{CRP}(\alpha) &
\beta_{k,l,p} &\sim \mbox{N}(\mu_p,\sigma_p^2)
\end{align*}
where for each pair of clusters $(z_i,z_j)$ we have a vector of parameters $\boldsymbol{\beta}_{z_i,z_j}$ that corresponds to the covariate vector $\mathbf{s}(t,i,j|\cdot)$.   Thus, the model of dynamics for the dyad $(i,j)$ has the same parameters as for other dyads occurring between group $z_i$ and $z_j$.  To allow for the blocks to share information by placing a hierarchical prior on each $\boldsymbol{\beta}_{k.l}$.

\subsection{Model specification}
  
% \begin{enumerate}
% \item Intercept: $s_{0}(t,i,j) = 1$
% \item Dyad count: $s_{1}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=i) )$
% \item Sender out-degree: $s_{1}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=i) )$
% \item Sender in-degree: $s_{2}(t,i,j) = f(\sum_{m:t_m<t} I(j_m=i) )$
% \item Receiver out-degree: $s_{3}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=j))$
% \item Receiver in-degree: $s_{4}(t,i,j) = f(\sum_{m:t_m<t} I(j_m=j))$
% \item Reciprocity (AB-BA): $s_{5}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$
% \item Turn-taking (AB-BY): $s_{6}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$
% \item Turn-usurping (AB-XA): $s_{7}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$
% \item Turn-usurping (AB-XB): $s_{8}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$
% \item Turn-continuing (AB-AY): $s_{9}(t,i,j) =  I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$
% %\item Number of shared two-paths: $s_{10}(t,i,j) =   \sum_{m:t_m<t} \sum_{r=1}^NI(i_m=i,j_m=r or i_m=r,j_m=j)$
% % \item Shared contacts (since last occurrence): $s_{ij10}(t,i,j) = \sum_{r=1}^N I(i_m=i,j_m=j,i_{v_{ijm}}=i,j_{v_{ijm}}=r)$
% % \item Triadic closure (since last occurrence): $s_{ij11}(t,i,j) = \sum_{r=1}^N I(i_m=i,j_m=j,i_{v_{ijm}}=r,j_{v_{ijm}}=j)$
% % \item Shared contacts (since last occurrence): $s_{ij12}(t,i,j) = \sum_{r=1}^N I(i_m=i,j_m=j,i_{v_{ijm}}=r,j_{v_{ijm}}=r)$
% \end{enumerate}

\begin{table}[t]
\footnotesize
\center
\begin{tabular}{|l|l|}
\hline
Statistic & Formula \\
\hline
\hline
Intercept:& $s_{0}(t,i,j) = 1$\\
Dyad count:& $s_{1}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=i) )$\\
Sender out-degree:& $s_{1}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=i) )$\\
Sender in-degree:& $s_{2}(t,i,j) = f(\sum_{m:t_m<t} I(j_m=i) )$\\
Receiver out-degree:& $s_{3}(t,i,j) = f(\sum_{m:t_m<t} I(i_m=j))$\\
Receiver in-degree:& $s_{4}(t,i,j) = f(\sum_{m:t_m<t} I(j_m=j))$\\
Reciprocity (AB-BA):& $s_{5}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}=i)$\\
Turn-taking (AB-BY): &$s_{6}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}}=j,j_{v_{ijm}}\ne i)$\\
Turn-usurping (AB-XA):& $s_{7}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}} \ne j,j_{v_{ijm}}=i)$\\
Turn-usurping (AB-XB): &$s_{8}(t,i,j) = I(i_m=i,j_m=j,i_{v_{ijm}} \ne i,j_{v_{ijm}}=j)$\\
Turn-continuing (AB-AY):& $s_{9}(t,i,j) =  I(i_m=i,j_m=j,i_{v_{ijm}}=i,j_{v_{ijm}}\ne j)$\\
\hline
\end{tabular}
\label{tab:stats}
\caption{Statistics used to specify intensity functions using the previous history $\mathcal{A}_t$.}
\end{table}



In Table \ref{tab:stats} we list the statistics we use in our experiments for  $\mathbf{s}(t,i,j)$.  Several statistics use $v_{mij} \in [0,M]$, the index of the last event where $(i,j)$ occurred (n.b. $t_{v_{mij}} = \tau_{mij}$). If $(i,j)$ has never occurred, we set $v_{mij}=0$.  Here we include degree effects to model possible rich-get-richer phenomenon.  For example, a particular node sending often may indicate they will continue to be a sender in the future.  We normalize these counts by the number of events up until a dyad's prior changepoint.\footnote{This seems to avoid ``explosion'' when simulating from the model.  TODO: Explore the theory about these sorts of statistics.  Aalen has a few suggestions.} Other statistics could be of interest for particular substantive questions \cite{Butts2008,Vu2011}.  \footnote{Describe the form of $f()$.  Right now it is $f(x) = \log \frac{x+1}{m + N(N-1)}$.}

The next set of statistics are \emph{participation shift} effects inspired by research in conversational norms \cite{Gibson2003}.  For example, an AB-BA effect indicates an increased propensity for reciprocity.  The final set of effects allow one to model phenomenon such as triadic closure.  Though we use only the above statistics in our experiments, one may use any quantity or set of covariates about a given dyad that is computed using the previous history of events.  The only restriction is that the statistic may not change in value between each observed event.\footnote{This prevents one from using the number of events occurring in some previous time window, as in \cite{Gunawardana2011}.}

\subsection{Relation to other models}

Our formulation is reminiscent of the stochastic blockmodel \cite{Nowicki2001,Kemp} which models the probability of a dyad as $p(y_{ij}) =\mbox{logit}^{-1}( \eta_{z_i,z_j})$ where $\eta_{z_i,z_j}$ is interpreted as a mixing rate between group $z_i$ and group $z_j$.  In the proposed method, however, the blockmodel structure facilitates the study of intra-group and inter-group dynamics via a continuous-time network model.

The above framework generalizes several important special cases. For example,  using only the intercept statistic $s_0(t,i,j) = 1$ is analogous to the stochastic block model for static networks.  Under this model each dyad is a homogeneous Poisson process and all dyad intensities $\lambda_{i,j}$ within block $(z_i,z_j)$ have the same intensity $\exp\{\boldsymbol{\beta}_{z_i,z_j}\}$.  As these intensities do not change under this specification, the likelihood simplifies to 
$$\mathcal{L}(\mathcal{A}|\beta) = \prod_{m=1}^M \lambda_{i_m,j_m} \prod_{(i,j) \in \mathcal{R}} \exp\{-t_M \lambda_{i,j}\}$$

Alternatively, if one models only the order of the events (ignoring the times at which they occur), the likelihood becomes a conditional logit
$$\mathcal{L}_{\mbox{mult}}(\mathcal{A}|\beta) = \prod_{m=1}^M \frac{\lambda_{i_m,j_m}(t_m | \cdot)}{\sum_{(i,j) \in \mathcal{R}} \lambda_{i,j}(t_m | \cdot)}.$$
The functional form is similar to conditional logit models used for discrete choice data \cite{McFadden1984}, though here the possible choices are all the dyads in $\mathcal{R}$.

\subsection{Related work}

Relational event models \cite{Butts2008} require a knot at each observed event, while other approaches such as \cite{Gunawardana2011} learn the regions where an intensity is constant and allow for a nonlinear relationship between statistics and intensity functions.  

(TODO: More here.  For example: \cite{Tillman2011} explores the idea of having nonpoisson processes at each edge and solves the master equation for the dynamical system.)

\section{Inference}

We use Markov chain Monte Carlo to sample from the posterior distribution of our parameters.  

\subsection*{Sampling $\mathbf{z}$ given $\boldsymbol{\beta}$ and $\mathcal{A}$ }

We Gibbs sample the latent class assignments $\mathbf{z}$ from the conditional distribution
\begin{align*}
p(z_i | z_{-i},\mathcal{A},\alpha,\boldsymbol{\beta}) \propto p(z_i | z_{-i},\alpha) p(\mathcal{A}|\mathbf{z},\boldsymbol{\beta},\mbox{node} \ i \ \mbox{involved}).% & z_i &= \mbox{new} = \alpha
\end{align*}
Letting $\mathcal{U}_r = \{a: r \in \{i_a,j_a\}, a \in [1,M]\}$ be the set of events where node $r$ is involved, the likelihood term above can be written as
$$p(\mathcal{A}|\mathbf{z},\boldsymbol{\beta},\mbox{node} \ r \ \mbox{involved}) = \prod_{a \in \mathcal{U}_r} \lambda_{i_a,j_a}(t_a|\cdot)
\prod_{(i,j) \in \mathcal{R}_{i_a,j_a}} \exp \{ -(t_a - \tau_{aij}) \lambda_{ij}(t_a|\cdot)\}$$

\noindent Under a CRP($\alpha$) prior, we have $p(z_i = k | z_{-i},\alpha) = n_k $ and $p(z_i = \mbox{new} |  z_{-i},\alpha) = \alpha$ where $n_k$ is the number of nodes assigned to cluster $k$. 
% \begin{align*}
% p(z_i = k | z_{-i},\alpha) &= n_k & p(z_i = \mbox{new} |  z_{-i},\alpha) &= \alpha
% \end{align*}


 By limiting the changepoints to times when either node is involved, computing the likelihood $p(A|z,\beta,\mbox{node} \ r \ \mbox{involved})$ is $O(|\mathcal{U}_r| \cdot P \cdot 4N)$.  The factor of 4 is due to the number of dyads involving a single node.  We precompute $\tau_{m,i,j}$ and $\mathbf{s}(t_m,i,j)$ for all $m,i,j$. 


\subsection*{Sampling $\boldsymbol{\beta}$ given $\mathbf{z}$ and $\mathcal{A}$ }

For each block $(k,l)$ we need to sample the vector of parameters $\boldsymbol{\beta}_{k,l}$ from its posterior
\begin{align*}
p(\boldsymbol{\beta}_{k,l} | \mathcal{A}_t, \textbf{z}, \mu, \sigma) &\propto p(\boldsymbol{\beta}_{k,l} | \mu, \sigma) p( \mathcal{A}| \textbf{z}, \boldsymbol{\beta}, \mbox{cluster} \ k \ \mbox{or} \ l \ \mbox{involved}) \\  
p(\boldsymbol{\beta}_{k,l} | \mu, \sigma) &= \prod_{p=1}^Pp(\beta_{k,l,p}|\mu_p,\sigma_p^2)
\end{align*}
the likelihood term above can be written as
$$p(\mathcal{A}|\mathbf{z},\boldsymbol{\beta},\mbox{cluster} \ k \ \mbox{or} \ l \ \mbox{involved}) = \prod_{a \in \mathcal{V}_{k,l}} \lambda_{i_a,j_a}(t_a|\cdot)
\prod_{(i,j) \in \mathcal{R}_{i_a,j_a}} \exp \{ -(t_a - \tau_{aij}) \lambda_{ij}(t_a|\cdot)\}$$
\noindent where $\mathcal{V}_{k,l} = \{a: z_{i_a} \in \{k,l\} \mbox{or} z_{j_a} \in \{k,l\}, a \in [1,M]\}$ is the set of events $a$ where one of the nodes is either in block $k$ or $l$.  \footnote{TODO: If we end up using HMC, mention step size and step length used.}
The above distribution is sampled using slice sampling.\footnote{We may also use HMC for this.}

\subsection*{Split-merge algorithm}

Motivation: simply Gibbs sampling does not seem to mix well.  \footnote{This section is planned future work.}

In the experiments we evaluate the performance of the model when restricting the number of clusters $K$.  For these chains we do not propose split moves when the number of non-empty clusters is already $K$.

% TODO: Update the following discussion about sampling.

% This operation can be done in parallel, and only the portion of the likelihood involving the newly assigned cluster  need to be recomputed.  Computing the likelihood is then $O(T \cdot P \cdot 4N)$ (though this computation is split into $K^2$ pieces) and must computed $N \cdot K$ times per Gibbs iteration.  The most straightforward implementation has space complexity $O(N^2)$ since we must keep track of $\tau_{i,j,m}$ for each $(i,j)$ at a given $m$.

% \subsection{Approximate likelihood methods}

% TODO: Discuss how one might sample the likelihood.  Cite Raftery paper.

% TODO: Look at performance of this with respect to bias and performance measure

\begin{figure}
\center
\includegraphics[width=1.6in]{../figs/synthetic/mat.pdf}
\includegraphics[width=2.25in]{../figs/synthetic/counts.pdf}
\includegraphics[width=2.5in]{../figs/synthetic/params-estimates.pdf}
\caption{Illustration of 1000 simulated events, as described in text. Left: Counts of each dyad. Center: Boxplot of distribution of participation counts across dyads.  The top left shows an increased propensity for reciprocity within cluster 1; bottom right shows more AB-AY events within cluster 2.  Right: Parameters (in red) and posterior credible intervals (in black).}
\label{fig:syncounts}
\end{figure}

\section{Simulation}

We check our model fitting procedure using a small synthetic data set involving 10 nodes from 2 clusters where 1) the first cluster has an increased tendency for reciprocity, 2) members of the second cluster have an increased tendency to continue speaking, and 3) interactions between groups are more likely to be repeated.  The specification of $\textbf{s}$ is therefore $s(t,i,j) = [s_0, s_{5}(t,i,j), s_{9}(t,i,j)]$.  For the synthetic data set we use parameter vectors $\boldsymbol{\beta}_{1,1} = (0,3,0)$,  $\boldsymbol{\beta}_{1,2} = \boldsymbol{\beta}_{2,1} = (-1,0,0)$, and $\boldsymbol{\beta}_{2,2} = (0,0,2)$.  This is done by sequentially computing $\lambda_{ij}(t_m|\cdot)$ for all $(i,j) \in \mathcal{R}$, drawing $t_{m+1}-t_m \sim \mbox{Exp}(\sum_{ij} \lambda_{ij}(t_m|\cdot))$, and drawing the dyad $(i,j) \sim \mbox{Categorical}(\lambda_{ij}(t_m|\cdot) / \sum_{ij}\lambda_{ij}(t_m|\cdot))$.  

In Figure \ref{fig:syncounts} we illustrate the simulated data.  While the dyad counts suggest a stochastic blockmodel, the center plot shows each block has empirical differences in their dynamics: intensities for reciprocal actions among actors in block 1 are $e^3$ times greater, intensities for turn-taking actions among actors in group 2 are $e^2$ times greater, and intensities for dyadic interactions between the two groups have a multiplicative effect of $e^{-1}$ and thus occur less often.  Fitting the model with $K=2$ converges to the true loglikelihood (see Table \ref{tab:results}) and the posterior credible intervals of the parameters cover the true parameter values.

\section{Model checking and experiments}

\small{
\input{../figs/results.tex}
}

\subsection*{Data}

Each of the following data sets are sequences of dyadic events, where each event has a time associated with it, a sender, and a recipient.

\begin{itemize}
%\item Email: In small organizations there may be groups of people who tend to have a similar style of interaction, and who also share a pattern of interaction with those outside the group or in another group.
\item Eckmann email:
\item Kiel email: Still need to hunt this down.
\item Enron email:
\item Tweets from Twitter.com occurring between from May 11, 2009 to January 26, 2012 that contained the hashtag \texttt{\#rstats}.\footnote{Will be made publicly available.}  This hashtag is used to denote messages pertaining to the R statistical computing environment and sometimes statistical discussion more generally.  We collect dyadic events by selecting tweets beginning with the \texttt{@} symbol (called a \emph{mention}), and mark the first mentioned user as the recipient.  Of 28337 total tweets in this time period, 3926 were directed events among a total of 1079 users.  We use the subset of 487 users who participated in more than one event, using a training set of 2000 events and a test set of 1330 events.
\item MIT Reality Mining: 
\end{itemize}

\subsection*{Prediction experiment}


\begin{figure}[th]
\caption{Recall plots showing predictive performance on a ranking task.  Left: Eckmann.  Right: Another dataset.  (TODO)}
\label{fig:recall}
\end{figure}



%\begin{figure}
%\center
%\includegraphics[width=3in]{../figs/synthetic/logposterior.pdf}
% \includegraphics[width=3in]{../figs/synthetic/test-recall.pdf}
% \caption{Results from fitting models to synthetic data. Left: Log posterior of 1000 simulated events vs. iteration during MCMC.  Right: Recall on 3000 test events.}
% \label{fig:synresults}
% \end{figure}
We evaluate the predictive ability of the fitted models by comparing models based on the loglikelihood of held-out data and recall on held out data.  Each data set is first split into a training set and a test set, and the loglikelihood of the test set is computed sequentially using Equation \ref{eqn:llk} where $\beta$ is set to be the mean from posterior samples given the training data.\footnote{This approach makes sense when the latent class assignments are not changing.  I will change this so that we averaging across predictions made from single draws from the posterior $\beta^{(i)}, z^{(i)}$.}

In addition, we compute recall to evaluate whether the next observed event is among the most likely according to the model.  At each event $m$ we sort the predicted intensities of all possible events in decreasing order, find the rank of the observed event in the list of predicted intensities, and compute the mean rank across the $M$ events.  

Several baselines are included for comparison: \texttt{uniform} places uniform probability on all possible dyads, \texttt{online} ranks events at time $t$ by the number of times the dyad has occurred previously $r_{online}(i,j) = \sum_{m:t_m < t} I(i_m=i,j_m=j)$, and \texttt{marginal} uses the product of the observed marginal frequencies $r_{marg} = \sum_{m:t_m < t} I(i_m=i) \sum_{m:t_m < t} I(j_m=j)$.  Note for processes that are homogeneous over time, \texttt{online} should do well with large amounts of data while \texttt{marginal} should roughly model heterogeneity in activity among individuals.  

Our method jointly models \emph{which} dyads occur and \emph{when} they occur.  To compare the above baselines to our model using the likelihood of observed data, we assume each dyad is a Poisson process with estimated  rate $\hat{\lambda}_{i,j} = \frac{M}{t_M} \frac{r_{b}(i,j) + \xi}{\sum_{ij} r_{b}(i,j) + \xi}$, where $r_b(i,j)$ is the statistic a baseline (described above) and $\xi=1$ is a smoothing parameter.

%We consider several variations of our model to investigate its properties. The \texttt{baserates} model only is an intercept only model (and mimics the behavior of a stochastic block model).   The \texttt{full} model uses all the statistics discussed in Section \ref{sec:modeladditional}.  For the \texttt{shared} model we force all diagonal blocks to use the same vector ($\boldsymbol{\beta}_{k_1,k_2} = \boldsymbol{\gamma}_1$ where $k_1=k_2$) and all non-diagonal blocks to use the same vector to be the same  ($\boldsymbol{\beta}_{k_1,k_2} = \boldsymbol{\gamma}_2$ where $k_1 \ne k_2$).  For each of these we may vary the number of clusters $K$. The \texttt{single} model uses all statistics but assumes a single cluster $K=1$. 

 Table \ref{tab:results} compares the train and test likelihood for each method and data set combination.  Two likelihoods are considered: 1) \texttt{rem} (relational event model) likelihood as given in Equation \ref{eqn:llk}, and \texttt{mult} (multinomial likelihood) given in Equation \ref{eqn:multllk}.  The latter only measures a model's predictive performance for \emph{what} occurs next, while the former also measures the ability to predict \emph{when} it occurs.  


For the synthetic data generated with $K=2$ clusters, the model with $K=2$ is best as expected.  Fitting the model with a single cluster also outperforms the simple baselines with respect to both likelihoods.  

For the Eckmann subset the fitted model with $K=2$ performs best except for the test data using the conditional logit likelihood.  This may be because the model has overfit the training set, or the sampler has not properly converged (as is the case with $K=3$).  \footnote{Will need to discuss results for other datasets once they are complete.}


Figure \ref{fig:recall} provides recall curves for several of the datasets.

\section{Discussion}

\begin{figure}[ht]
\includegraphics[width=3in]{../figs/eckmann-small/params-estimates}
\caption{Left: Posterior credible intervals for parameter estimates from fitting the $K=2$ model to the Eckmann subset.  Right: Posterior distribution of $K$ when fitting the model and allowing for flexible $K$.}
\label{fig:posteriorparams}
\end{figure}

 We propose a hierarchical approach for modeling event-based network data that is analogous to recent hierarchical extensions for latent position models \cite{Handcock2007} and exponential random graph models \cite{Schweinberger2011}.  The method combines a latent variable framework (stochastic blockmodels) with a local dependence model for event sequences (relational event models) to provide detailed models of event dynamics among subsets of nodes while simultaneously allowing for heterogeneity in dynamics of the network as a whole.

%By combining stochastic blockmodels with a relational event framework, the proposed method accounts for unobserved heterogeneity in the way dyadic events occur conditioned on the past. 

Figure \ref{fig:posteriorparams} shows examples of model estimates from data sets of dyadic interactions. Panels contain posterior credible intervals for parameters pertaining to dyads belonging to a particular block.

In Section \ref{sec:experiments} we show the model has improved predictive accuracy over baseline methods with respect to ranking tasks and the likelihood of unobserved data.



TODO: Discuss experimental results.  Describe qualitative behavior of each model's recall line.

TODO: Describe and include posterior predictive checks.

Other theories could be explored by including relevant statistics in the specification of $\mathbf{s}(t,i,j)$.

Our method is able to reveal heterogeneity that exists among individuals.  
The purpose of our method is to learn about event dynamics within latent groups of individuals.  Other types of heterogeneity likely exist in some data sets.  For example, dynamics might change over time \cite{Vu2011}.  Alternatively, if nodes change groups.  One approach, analogous to \cite{Airoldi2008}, would allow the latent class $z_i$ to be drawn from node-specific membership vectors $\pi_i$  after each change point.  In some contexts such extensions might be substantively important and warrant future work.


% \subsection{Extensions}
% Possible future work could extend the present framework to a mixed-membership model where for each effect there is a latent cluster assignment.  This way a group of nodes might contact high in-degree nodes at a similar rate, though they vary in the degree to which they adhere to reciprocity.  As in the mixed-membership stochastic blockmodel \cite{Airoldi2008,Shafiei2010}, one could assume that each node has vector of probabilities, $\boldsymbol{\pi}_i$, for being assigned to each of the latent clusters.  In that case $z_i \sim \mbox{Categorical}(\boldsymbol{\pi}_i)$ for each node $i$ and $j$, and the ties are modeled via a stochastic blockmodel $\mbox{logit} p(y_{ij}) = \eta_{z_i,z_j}$.  One could extend our current framework by assuming that for each event $m$, any node  $i,j \in \mathcal{R}_{i_m,j_m}$ redraws their latent class.  TODO: Discuss why we *didn't* do this.

% Possible variants:
% \begin{itemize}
% \item Discuss possible variations on the prior structure of $\beta$: 1. Enforce $\beta_{k,l} = \beta_{l,k}$. 2. Assume $\beta_{k,l,p} \sim \mbox{Normal}(\beta_p,\sigma_p^2)$. 3. Place other priors that provide different kinds of regularization. 
% \item $\log \lambda_{ij} = (\pi_i \cdot \pi_j) \eta s(t,i,j)$
% \item  HMM that guides which latent class over time?
% \item  HDP instead of fixed K: Hierarchical, nonparametric model that allows parameters to be shared across blocks.
% \item different clusters for senders and receivers (cite our KDD paper)?
% \item More statistics, e.g. topic model similarity
% \end{itemize}



% \appendix
% \section{Simulation}

% Simulation from the model follows the typical racing exponentials idea with a few subtleties.

% \begin{enumerate}
% \item $t_0 = 0$, all $s_{ij} = 1$, Initialize $v = 0$
% \item For $i = 1, \ldots, N$: Draw $z_i \sim \mbox{Categorical}(\theta)$
% \item For $m = 1, \ldots, M$:
%   \begin{enumerate}
%   \item Draw $t_m \sim \mbox{Exponential}\left(\sum_{ij} \lambda_{ij}^*\right)$
%   \item Draw $(i_m,j_m) \sim \mbox{Categorical}\left(\lambda_{ij}^* / \sum_{ij}\lambda_{ij}^*\right)$
%   % \item Save $s_{ij}(t_m) \leftarrow s_{ij}^*$
%   % \item Save $\lambda_{ij}(t_m) \leftarrow \lambda_{ij}^*$
%   \item For all $(i,j)$ where $i$ or $j$ is in $\{i_m,j_m\}$:
%     \begin{enumerate}
%     \item Compute $s_{ij}^*$ using event $0$ through $v_{ij}$
%     \item Set $\lambda_{ij}^* \leftarrow \exp\{ \beta_{z_i,z_j} s_{ij}^*\}$ 
%     \end{enumerate}
%   \end{enumerate}
% \end{enumerate}

% \section{EM for baseline}

% In our baseline model, each block $(a,b) \in [1,K] \times [1,K]$ simply has a rate $\gamma_{a,b}$.
% The likelihood is
% \begin{align*}
% \mathcal{L}(A|\lambda) &= \prod_{m=1}^M \lambda_{i_m,j_m} \prod_{(i,j) \in \mathcal{R}} \exp\{-t_M\lambda_{ij}\} \\
% & =\prod_{a=1}^K \prod_{b=1}^K \prod_{m=1}^M \gamma_{ab}^{I(z_{i_m}=a,z_{j_m}=b)} \prod_{(i,j): z_i=a,z_j=b} \exp\{-t_M\gamma_{ab}\} \\
% & =\prod_{a=1}^K\prod_{b=1}^K \gamma_{ab}^{n_{ab}} \exp\{-t_Mn_an_b\gamma_{ab}\}
% \end{align*}
% where $n_{ab} = \sum_m I(z_{i_m}=a,z_{j_m}=b)$, $n_a = \sum_{i=1}^N I(z_i=a)$.

% The loglikelihood is $$\ell(A|\lambda) = \sum_{a=1}^K \sum_{b=1}^K n_{ab} \log \gamma_{ab} - t_M n_a n_b \gamma_{ab}$$
% and so $\hat{\gamma}_{ab} = \frac{n_{ab}}{t_Mn_an_b}$.


% I do not believe it is possible to perform EM for a model where we have the more general linear predictor for the intensity function.  Since the assignment of one node depends on the assignments of the other nodes, we suffer combinatorial explosion.

% \section{Gradient}

% \begin{align*}
% \frac{\partial \ell}{\partial \beta_{p,k,l}} &= \frac{\partial}{\partial \beta_{p,k,l}} \sum_{m = 1}^M \left[ \log \lambda_{i_m,j_m}(t_m | \cdot) - \sum_{(i,j) \in \mathcal{R}_{i_m,j_m}} (t_m-\tau_{i,j,m}) \lambda_{i,j}(t_m|\cdot) \right] \\ 
%   &=\sum_{m \in V} \left[ \frac{\partial}{\partial \beta_{p,k,l}} \sum_p \beta_{p,k,l} s_p(t,i,j|\mathcal{A}_t)  - \sum_{(i,j) \in \mathcal{R}_{i_m,j_m}} (t_m-\tau_{i,j,m}) \frac{\partial}{\partial \beta_{p,k,l}} \exp \left\{\sum_p \beta_{p,k,l} s_p(t,i,j|\mathcal{A}_t) \right\} \right] \\
%   &=\sum_{m \in V} \left[ s_p(t_m,i_m,j_m) - \sum_{(i,j) \in \mathcal{R}_{i_m,j_m}} (t_m-\tau_{i,j,m}) s_p(t_m,i,j) \lambda_{i,j}(t_m|\cdot) \right]
% \end{align*}

% where $V$ is the set of $m$ where either $z_i \in \{k,l\}$ or $z_j \in \{k,l\}$.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
