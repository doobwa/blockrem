\section{Model fitting and experiments}
\label{sec:experiments}

%\subsection{Data}

A variety of real world data sets are used to explore the efficacy of the model.  The following data sets are sequences of dyadic events, where each event has a sender, recipient, and timestamp.  For each data set, we hold out the final $M_{test}$ events for evaluation of the model.
\begin{list}{\labelitemi}{\leftmargin=1em}
%\begin{itemize}
%\item Kiel email: \cite{Ebel2002}
\item Classroom \cite{McFarland2001}: 445 directed communication among 27 people a high
  school classroom collected via participant observation ($M_{test}= 145$).
\item University email \cite{Eckmann2004}: 3300 dyadic emails among 88
  users with at least 30 emails ($M_{test} = 1300$).
\item Enron email \cite{Klimt2004}: 4000 dyadic emails among 141
  individuals between July 2001 and August 2001 ($M_{test} = 1000$).
%\item Irvine: dyadic interactions among 401 actors each having more than 30 events,  \cite{Opsahl}
\item Twitter direct messages: Tweets from Twitter.com occurring between from May 11, 2009 to January 26, 2012 that contained the hashtag \texttt{\#rstats}.  This hashtag is used to denote messages pertaining to the R statistical computing environment and sometimes statistical discussion more generally.  We collect dyadic events by selecting tweets beginning with the \texttt{@} symbol (called a \emph{mention}), and mark the first mentioned user as the recipient.
Of 28337 total tweets in this time period, 3926 were directed events among a total of 1079 users.
We use a subset 4330 events among 487 users who participated in more than one event ($M_{test} = 1330$).\footnote{This dataset will be made publicly available.}
\item MIT Reality Mining \cite{Eagle2009}: 2000 phone calls among the
  89 recipients between October 2001 and February 2002 ($M_{test}= 1000$).
%\end{itemize}
\end{list}

\subsection{Model-based exploratory analysis}

Figure \ref{fig:parmats} uses a fitted model to provide an example of the differences in the dynamics that can exist between two groups of dyads with similar rate of occurrence.
We focus on the interactions between members of block 1 to members of block 3; 179 events occurred originating from a member of group 3 and 178 originated from group 1.
For this example we fit a model with an intercept $s_0$, $\texttt{ab-ba}$ effects $s_1$, and $\texttt{ab-by}$ effects $s_3$.
The two panels on the left show the variation in $s_0(t_M,i,j,\mathcal{A}_t)$, $s_1(t_M,i,j,\mathcal{A}_t)$ and $s_3(t_M,i,j,\mathcal{A}_t)$ across the dyads $(i,j)$ in blocks $(1,3)$ and $(3,1)$, respectively.
The boxplots on the right describe the posterior samples of the corresponding parameters $\beta_{1,3,p}$ and $\beta_{3,1,p}$.
Under the model, events from group 3 to group 1 occur roughly $e^1$ times as often.
Similarly, block (3,1) has a higher propensity for \texttt{ab-by} transitions under the model.
The model has learned that this structure exists in the data, and furthermore our inferences from the parameter estimates are sensible given the observed statistics.

\subsection{Prediction experiments}

We evaluate the predictive ability of the fitted models by comparing models based on the loglikelihood and recall for held-out data.
%Each data set is first split into a training set and a test set, and 
The loglikelihood of the test set is computed sequentially using Equation \ref{eqn:llk} where $\hat{\lambda}_{i,j}(t_m) = \frac{1}{L}\sum_l \lambda_{i,j}(t_m | \boldsymbol{\beta}^{(l)}, \mathbf{z}^{(l)},\mathcal{A}_t)$ is averaged using $L$ posterior samples given the training data.
% TODO: We compute both the relational event model likelihood  (\texttt{rem}) as given in Equation \ref{eqn:llk}  and the multinomial likelihood (\texttt{mult}) given in Equation \ref{eqn:multllk}.
% TODO: The latter only measures a model's predictive performance for \emph{what} occurs next, while the former also measures the ability to predict \emph{when} it occurs.

In addition, we compute recall to evaluate whether the next observed event is among the most likely according to the model.
At each event $m$ we sort the predicted intensities of all possible events in decreasing order, find the rank of the observed event in the list of predicted intensities, and compute the mean number of events ranking above cutoffs $\kappa=5$ and $\kappa=20$. 

\input{../figs/results-llk4.tex}
\input{../figs/results-recall-all.tex}

Several baselines are included for comparison: \texttt{uniform} places uniform probability on all possible dyads, \texttt{online} ranks events at time $t$ by the number of times the dyad has occurred previously $r_{online}(m,i,j) = \sum_{m:t_m < t} I(i_m=i,j_m=j)$, and \texttt{marginal} uses the product of the observed marginal frequencies $r_{marg}(m,i,j) = \sum_{m:t_m < t} I(i_m=i) \sum_{m:t_m < t} I(j_m=j)$.  
Finally, \texttt{BM} is a stochastic blockmodel (i.e. our model with only an intercept term).
Note for processes that are homogeneous over time, \texttt{online} should do well with large amounts of data while \texttt{marginal} and \texttt{BM} can capture individual heterogeneity and group level heterogeneity in overall activity. 

Our method jointly models \emph{which} dyads occur and \emph{when} they occur.
To compare the above baselines to our model using the likelihood of observed data, we assume each dyad is a Poisson process with estimated  rate $\hat{\lambda}_{i,j}(t_m) = \frac{M}{t_M} \frac{r_{b}(i,j) + \xi}{\sum_{ij} r_{b}(i,j) + \xi}$, where $r_b(i,j)$ is the statistic a baseline (described above) and $\xi=1$ is a smoothing parameter.

In order to investigate the role of the number of clusters, we introduce an upper bound $K^*$ on the number of clusters during the fitting process.  In Table ~\ref{tab:results} we compare the loglikelihood on held-out test data while varying  $K^*$.  A relational event model (i.e. our model with $K=1$) outperforms the baselines on these data sets.  For the synthetic data example introduced in Section \ref{sec:simulation} the fitted models with $K>1$ have predictive performance comparable to the true model; a standard relational event model does not perform as well because it does not have the flexibility to model the dynamics of each block separately.  For the classroom data set we see only a slight improvement with $K>1$, while with the mobile phone calls the $K=3$ model performs best.
In Table~\ref{tab:recall20} we include the corresponding results for the recall experiment at a cutoff of 5 and 20, respectively.  The results there are similar.

% \begin{figure}[t]
% \center
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \caption{Recall plots showing predictive performance on a ranking task.  Left: Eckmann.  Right: Another dataset.  (TODO)}
% \label{fig:recall}
% \end{figure}


% Table \ref{tab:results} compares the train and test likelihood for each method and data set combination.  For the synthetic data generated with $K=2$ clusters, the model with $K=2$ is best as expected.  Fitting the model with a single cluster also outperforms the simple baselines with respect to both likelihoods.

%For the Eckmann subset the fitted model with $K=2$ performs best except for the test data using the conditional logit likelihood.  This may be because the model has overfit the training set, or the sampler has not properly converged (as is the case with $K=3$).  \footnote{Will need to discuss results for other datasets once they are complete.}
