* TODO Week 1:
- [X] Need to change block.ps to instead look at dyad-specific p-shifts.
[done?] Use a visualization of these p-shifts to explore the properities of the synthetic data.
- [X] Fix llk, writeup to account for sender/receiver for both i and j
- [X] Fix pshift to account for self link issue
- [X] Clean up write up and send to Padhraic.  
- [X] Get sense of size of dataset we can use.
- [X] Write mle for dyad-indpenent REM model.  (Double check llk function.)
- [X] Write sampler for blockmodel.
- [X] Fit blockmodel to synthetic data.

* TODO Week 2: 
- [X] Write SBM code.
- [X] Fit SBM to subset of Eckmann (dyadic only)
- [X] Fit REM separately to each block.
- [X] Implement BREM for submodels by modifying MH call in MCMC.
- [X] Setup prediction llk experiment and ranking code. Handle case where we have draws from posterior.
- [X] Setup experiment comparing submodels on synthetic data: k=1 and just intercept, k=1, beta for diag and gamma for off diag, and full model.
[done. not successful.] Noncollapsing clusters: try initializing beta centered on the mean overall rate.
- [X] Download some twitter data
- [X] Explore performance of approximate likelihood. [Needs roughly N/2 to be good in simple examples.]
Debug issues with whether to include intercept term or not. (Understand collinearity/identifiability issue.)
- [X] Reimplement brem.llk to take account of affected (i,j) that aren't in block (a,b).
- [X] Reimplement brem code with Arrays
- [X] Understand why we do better than truth on llk/ranking. [It was a bug in llk.]
[done, still buggy] Make faster llk function that doesn't use lrm w/ llk.
[done. overhead too big?] Try to speed up with OpenMP
- [X] Implement gibbs sampler for latent group assignment.
- Pass tests with one that computes mp ahead of time.
- Determine what's slowing things down.  Overhead?

* TODO Week 3:
- [X] Add features for saving progress during MCMC
- [X] Clean up experiment code so that you can fit all needed models on arbitrary dataset
- [X] Pull out code for plotting results from experiment
- [X] Compare submodels for eckmann subset with K=2. [online baseline was much better than BREM.]
- [X] Include degree effects (transform: sqrt? log?)
- [X] Get test suite to display results on all test scripts.
- [X] Make into a package.
* TODO Week 4:
- [x] Try eckmann-small and synthetic dataset with degree effects.  
  - [x] Tinker with simulating with degree effects.  Perhaps come up with
    other parameterization that is less sensitive to explosion.
- [x] Implement slice sampling (to hopefully improve mixing of parameters)
- [x] Understand why baseline beats a model that only has dyad count
  effects.  [ans: dyad counts in stats vector are "stale"]
- [x] Change parameterization of dyad count (so that we beat online
  baseline).  [tried sqrt in the denominator and it did not improve.]
- [x] add prediction on training data to dashboard
- [x] Get twitter training and test set up
- [X] Add UC Irvine dataset
- [X] See if we can learn groups on synthetic data.
- [x] Why is synthetic fit making the dyad count effect so large? [wrong
  synthetic dataset?] [ans: with K=1 the model needs to make dyad
  count effect large and make the other degree effects small.  with
  K>1 this should not happen...]
- Rerun synthetic data without degree effects?  
  - [x] fix bug in synthetic fitting? [bug in slice sampling always used
    shared model.]
  - [x] nonidentifiability with degree effects and dyad counts? [Don't
    believe so, ubt still seem to have some interactions between
    degree effects and intercept, for example.  perhaps this only
    occurs when we have small amounts of data (e.g. off diagonal
    blocks in synthetic example)]
  - [x, but still occasionally shoots to negative values.  I now think
    this is due to nonidentifiability... e.g. when I leave an
    intercept term in with K=1 and just pshifts] slice sampling seems to work better when K=1, but worse when K>1.
    Seems to break occasionally and go to extreme negative values.
  - MH seems to work better than slice sampling on shared model with K=2 and large P.  This might be because of highly correlated variables. Needs longer burn in though.  Not using (1,1,1) variable (intercept for block 1,1) seems to help.
  - Bug in slice sampling since it requires global variables
- [x] Fix prediction code so that it doesn't take as much memory and is fasters.
- [x] Run all datasets with K=1 and get dashboard
- Work on better proposal dist. and better mixing of group assignments.
  - Split/merge moves: Try split moves where we select a subset and offer them a new
    cluster with the same parameters as the previous cluster.  Try an
    experiment with this method where we have 2 true clusters and
    initialize everyone in one cluster.
  - Or just use tons of chains with few iterations?
- [X] Add other statistics other than pshifts
  - Implement shared partners in statistics.
- [x] Understand why we outperform true model in synthetic example
  (i.e. this is a bug and wasn't there before) (see if it's still present)
* TODO Week 5
- [x] Learn twitter z's with K=2.  Compare to fixed z's.  Do we learn to
  put users into high activity group / low activity grou;?  [No.]
- [x] Implement loglik. for baselines and compare to models' train/test
  llk.
  - debug why online's numbers are so weird.
- [x] Examine where we obtain errors.  
  - for new users on test set?  [doesn't seem to be the case]
  - near beginning of test set or end? [not really.  see email to
    padhraic on eckmann data]
- [x] Compute multinomial score for all tasks
- [x] Get slice sampling working with fixed z and learning z on
  twitter.  Seems that we weren't escaping initialization with MH.
  - [x] see if we have significatnly different parameters [seems to]
  - [x] slice sampling works better without AB-AB effect? [seems to]
- [x] fix  bug in uniform/marginal llks
- [x] Reduce memory for prediction task
- [x] Compare baserates model for eckmann, synthetic
- Look closely at predictions of online model to see why test llk is
  so high.
- Compare log prob score of online vs brem with K=1
- Compare loglikelihood of homogeneous poisson process to sums of our
  estimated rates.
  - compare to smoothed rates.
  - compare to observed counts.
- Implement prediction task for those we haven't seen in dashbaord.
- Add statistic indicating if something's been seen before (dc > 1)
- Fix baserates MCMC
- Add baseline for number of shared partners
- Try and come up with heuristics that get us to good places in the posterior
- Examine nonstationarity of dataset by looking at how snapshots
  change over time
- If I take one of the learned z's and fix them, do we learn more
  distinct beta's? [doesn't seem to be the case]
- Try prediction task where the test set is smaller
- 


* Other
- Add tests for Gibbs equations.  For example, start with true data
  and learn z's.
- Add tests for synthetic fitting?
- Derive/check identifiability
- Implement baseline that ranks by number of shared partners.
- Create prediction task for events that have not occurred yet.
- [X] Type up Gibbs sampling equations.
- Include random effect options.  Compute_lambda just needs to know
  where to look in beta vector.  Proposal needs to not suggest that
  there are K^2 of these parameters, instead just one set shared
  across the various latent groups.
- Log likelihood baseline that only knows timing effects.
- Write abstract.
- Add Enron.
- Add px feature to define which parameters to fit.
- Derive/implement EM for baseline model.
- Write up other possibilities for layering additional types of data, DP, or other extensions.
- Try random groups and see if REM still picks out (significantly) different parameters
- Consider MIT dataset or classroom dataset.
- Clean up interfaces to Rcpp code.  
  - Put all constants in RemStat.  Take out of ComputeLambdaFast, etc.
- Document Rcpp code better.
- Initialize with guess at z's, rather than guess at beta.
- Research maximization of beta rather than sampling.

* 
* Low priority:
- Port simulator over to Rcpp.

* Bugs:
- old likelihood does not use the baserates when computing first event for the likelihood computation
