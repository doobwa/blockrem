* TODO Week 1:
- [X] Need to change block.ps to instead look at dyad-specific p-shifts.
[done?] Use a visualization of these p-shifts to explore the properities of the synthetic data.
- [X] Fix llk, writeup to account for sender/receiver for both i and j
- [X] Fix pshift to account for self link issue
- [X] Clean up write up and send to Padhraic.  
- [X] Get sense of size of dataset we can use.
- [X] Write mle for dyad-indpenent REM model.  (Double check llk function.)
- [X] Write sampler for blockmodel.
- [X] Fit blockmodel to synthetic data.

* TODO Week 2: 
- [X] Write SBM code.
- [X] Fit SBM to subset of Eckmann (dyadic only)
- [X] Fit REM separately to each block.
- [X] Implement BREM for submodels by modifying MH call in MCMC.
- [X] Setup prediction llk experiment and ranking code. Handle case where we have draws from posterior.
- [X] Setup experiment comparing submodels on synthetic data: k=1 and just intercept, k=1, beta for diag and gamma for off diag, and full model.
[done. not successful.] Noncollapsing clusters: try initializing beta centered on the mean overall rate.
- [X] Download some twitter data
- [X] Explore performance of approximate likelihood. [Needs roughly N/2 to be good in simple examples.]
Debug issues with whether to include intercept term or not. (Understand collinearity/identifiability issue.)
- [X] Reimplement brem.llk to take account of affected (i,j) that aren't in block (a,b).
- [X] Reimplement brem code with Arrays
- [X] Understand why we do better than truth on llk/ranking. [It was a bug in llk.]
[done, still buggy] Make faster llk function that doesn't use lrm w/ llk.
[done. overhead too big?] Try to speed up with OpenMP
- [X] Implement gibbs sampler for latent group assignment.
- Pass tests with one that computes mp ahead of time.
- Determine what's slowing things down.  Overhead?

* TODO Week 3:
- [X] Add features for saving progress during MCMC
- [X] Clean up experiment code so that you can fit all needed models on arbitrary dataset
- [X] Pull out code for plotting results from experiment
- [X] Compare submodels for eckmann subset with K=2. [online baseline was much better than BREM.]
- [X] Include degree effects (transform: sqrt? log?)
- [X] Get test suite to display results on all test scripts.
- [X] Make into a package.
* TODO Week 4:
- [x] Try eckmann-small and synthetic dataset with degree effects.  
  - [x] Tinker with simulating with degree effects.  Perhaps come up with
    other parameterization that is less sensitive to explosion.
- [x] Implement slice sampling (to hopefully improve mixing of parameters)
- [x] Understand why baseline beats a model that only has dyad count
  effects.  [ans: dyad counts in stats vector are "stale"]
- [x] Change parameterization of dyad count (so that we beat online
  baseline).  [tried sqrt in the denominator and it did not improve.]
- [x] add prediction on training data to dashboard
- [x] Get twitter training and test set up
- [X] Add UC Irvine dataset
- [X] See if we can learn groups on synthetic data.
- [x] Why is synthetic fit making the dyad count effect so large? [wrong
  synthetic dataset?] [ans: with K=1 the model needs to make dyad
  count effect large and make the other degree effects small.  with
  K>1 this should not happen...]
- Rerun synthetic data without degree effects?  
  - [x] fix bug in synthetic fitting? [bug in slice sampling always used
    shared model.]
  - [x] nonidentifiability with degree effects and dyad counts? [Don't
    believe so, ubt still seem to have some interactions between
    degree effects and intercept, for example.  perhaps this only
    occurs when we have small amounts of data (e.g. off diagonal
    blocks in synthetic example)]
  - [x, but still occasionally shoots to negative values.  I now think
    this is due to nonidentifiability... e.g. when I leave an
    intercept term in with K=1 and just pshifts] slice sampling seems to work better when K=1, but worse when K>1.
    Seems to break occasionally and go to extreme negative values.
  - MH seems to work better than slice sampling on shared model with K=2 and large P.  This might be because of highly correlated variables. Needs longer burn in though.  Not using (1,1,1) variable (intercept for block 1,1) seems to help.
  - Bug in slice sampling since it requires global variables
- [x] Fix prediction code so that it doesn't take as much memory and is fasters.
- [x] Run all datasets with K=1 and get dashboard
- Work on better proposal dist. and better mixing of group assignments.
  - Split/merge moves: Try split moves where we select a subset and offer them a new
    cluster with the same parameters as the previous cluster.  Try an
    experiment with this method where we have 2 true clusters and
    initialize everyone in one cluster.
  - Or just use tons of chains with few iterations?
- [X] Add other statistics other than pshifts
  - Implement shared partners in statistics.
- [x] Understand why we outperform true model in synthetic example
  (i.e. this is a bug and wasn't there before) (see if it's still present)
* TODO Week 5
- [x] Learn twitter z's with K=2.  Compare to fixed z's.  Do we learn to
  put users into high activity group / low activity grou;?  [No.]
- [x] Implement loglik. for baselines and compare to models' train/test
  llk.
  - debug why online's numbers are so weird.
- [x] Examine where we obtain errors.  
  - for new users on test set?  [doesn't seem to be the case]
  - near beginning of test set or end? [not really.  see email to
    padhraic on eckmann data]
- [x] Compute multinomial score for all tasks
- [x] Get slice sampling working with fixed z and learning z on
  twitter.  Seems that we weren't escaping initialization with MH.
  - [x] see if we have significatnly different parameters [seems to]
  - [x] slice sampling works better without AB-AB effect? [seems to]
- [x] fix  bug in uniform/marginal llks
- [x] Reduce memory for prediction task
- [x] Compare baserates model for eckmann, synthetic

* Final crunch for deadline
- Look closely at predictions of online model to see why test llk is
  so high.
- Compare log prob score of online vs brem with K=1
  - look only at events from group 3 to group 1 with K=3 fit.  see
    which events may be leading to overfitting. 
  - see if those likelihood numbers are justified.  
  - [x] perhaps the initial events are throwing things off for the
    models.  perhaps it "works" really hard at fitting those first few
    events. [this idea didn't pan out.  we do an OK job of fitting the
    time for the first few.
  - Strangeness: multinomial likelihoods for trained modle bunched
    around a few values.  For example, many lambda_ij's are 0 even
    much later in the event sequence.

* Week 7
- Debugging synthetic example:
  - Identifiability and intercept terms?
  - Fixing intercepts and degree effects: slice converges quickly
  - Conclusion: intercepts and pshifts are highly correlated and in the
    synthetic example the degree effects are highly correlated.
    Univariate sampling is working correctly, just slowly.
- Posterior predictive plots: 
  - degree dists
  - [x] time plot
  - [x] pshifts
- [x] To discuss with Smyth:
  - plan for sharing parameters better
  - ppc's "need" to condition on prior observations
  - need fewer parameters if we want to do this in any reasonable
    amount of time....  
  - HMC?
- Sample just one block
  - with MH
  - with HMC
  - compare speeds
- Fix shared model to have different within block dynamics
- Implement online prediction that doesn't need to store full log
  intensity arrays (or does it piecemeal)
- Explosion guarantees?
- Get synthetic example figures completed
 - show parameter values vs posterior dist.
- Bug: Synthetic example: last observation not equal for
  RemLogLikelihoodPc and RemLogLikelihood.
- Reorg: Put precomputing inside of brem.mcmc
- Reorg: change get.pred to predict() 


- [x] Compare loglikelihood of homogeneous poisson process to sums of our
  estimated rates.
  - compare to smoothed rates.
  - compare to observed counts.
- Implement prediction task for those we haven't seen in dashbaord.
- Add statistic indicating if something's been seen before (dc > 1)
- Fix baserates MCMC
- Add baseline for number of shared partners
- Try and come up with heuristics that get us to good places in the posterior
- Examine nonstationarity of dataset by looking at how snapshots
  change over time
- If I take one of the learned z's and fix them, do we learn more
  distinct beta's? [doesn't seem to be the case]
- Try prediction task where the test set is smaller
- 

* Other
- Add tests for Gibbs equations.  For example, start with true data
  and learn z's.
- Add tests for synthetic fitting?
- Derive/check identifiability
- Implement baseline that ranks by number of shared partners.
- Create prediction task for events that have not occurred yet.
- [X] Type up Gibbs sampling equations.
- Include random effect options.  Compute_lambda just needs to know
  where to look in beta vector.  Proposal needs to not suggest that
  there are K^2 of these parameters, instead just one set shared
  across the various latent groups.
- Log likelihood baseline that only knows timing effects.
- Write abstract.
- Add Enron.
- Add px feature to define which parameters to fit.
- Derive/implement EM for baseline model.
- Write up other possibilities for layering additional types of data, DP, or other extensions.
- Try random groups and see if REM still picks out (significantly) different parameters
- Consider MIT dataset or classroom dataset.
- Clean up interfaces to Rcpp code.  
  - Put all constants in RemStat.  Take out of ComputeLambdaFast, etc.
- Document Rcpp code better.
- Initialize with guess at z's, rather than guess at beta.
- Research maximization of beta rather than sampling.

* Low priority:
- Port simulator over to Rcpp.

* Bugs:
- old likelihood does not use the baserates when computing first event for the likelihood computation

* Restarting
- [X] Clean up some files
- [ ] Shift samplers to use qmcmc package
- [ ] Get sampler tests and gibbs tests to pass
- [X] Possible bug in split merge:  shouldn't split look like initial?
- [ ] Bug in sample_phi: restricted range for k1 and k2
- [ ] 

* Final week
- [X] get synthetic example working again with new sampler
- [X] fix sample_phi to use block() for computing lposterior
- [X] why does lposterior not have alpha in it?
- [X] Correctness of Gibbs sampler and ActorPc function
Make sure that LlkActor(z_i=1) - LlkActor(z_i=2) = Llk(z_i=1)-Llk(z_i=2)
- [ ] Speed up synthetic data example
  K=2, 3 extra.  N=10.  M=2000
  - w/ block phi:   8-12 secs for 5 iterations
  - w/ actor gibbs: 8 secs now.  
  - appears that slice sampling is the majority of the time.
  - [ ] use precomputed llk as input to slice sampling.
  (seems to cause problems with slice sampler... probably not correct)
  starting llk)
- [X] Likelihood computations working with ego restriction
- [X] Synthetic example fitting correctly
  - [X] check bias plot.  [Works with known mu, sigma.]
- [-] Pipeline on synthetic data set with online prediction
  - [X] online evaluation of baselines
  - [ ] make sure pipeline works and spits out results table
- [-] BREM function (that uses splitmerge)
  - [X] return ego in BREM fit object
  - [X] Role of priors?  Hierarchical portion?
  - [X] Learn mu and sigma?
  - [X] Bug in llk_node maybe keeps z's from being sampled? Give warning
  - [X] transform degree effects?
  - [ ] Fixed K vs DP option
  - [ ] K=1 experiments.  (Bug in K=1 setup?)
  - [ ] Int only, pshift, pshift and degree options
  - [ ] Int only experiments
  - [ ] Sample without integrating out sigma.  Improve deg models?
  - [ ] Make sure new clusters from upper level mu and sigma
  - [ ] Take out mention of K=2 accuracy
  - [ ] Move all hyperparameter discussions to the end of Section 3
  - [ ] Simulation (as in stochastic simulation of ...)
  - [ ] Average over predictions
  - [ ] Results section
  - [ ] Posterior predictive instead of Figure 5
  - [ ] Fix title
  - [X] Allow different priors on different effects p?
- [X] Double check pshift implementation; is it global? 
- [X] Set up reality mining
- [X] classroom example
- [X] Set up enron dataset
- [ ] Split-merge attempts
- [ ] implement recency stats
- [ ] Fit Eckmann and Twitter (w/o pshifts)
- [ ] Compare llk timing to Carter's code
* Paper
- [X] Fix up notation borrowing notation from HierRelEvent paper.
- [-] Ask Padhraic: 
  - [X] I've followed the advice Carter gave for the other paper and
    written the likelihood up until a particular time t, which forces
    us to include an additional term between the last observed time
    and t.  Makes things a bit more visually complicated unfortunately.
  - [ ] Do you have a better way of writing A_t?  I need to represent that
    the time is ordered, that there are M events, and that each (i,j)
    is in R.  But I also want to use A_t in Equation 1, but there the
    events are only times without dyads; this is so that the reader
    can start with a single poisson process, then move to N^2 of them.
  - [X] Should I describe how our restriction makes things faster in
    Section 2, or in an appendix?  We can describe it quickly in a
    sentence or two, but the notation might be a bit involved and
    requires rewriting the likelihood and whatnot.  For example, I
    would write the likelihood using a variable that refers to the
    last changepoint for a given dyad.   Or should we use that
    notation throughout?  
- [X] Fix Gibbs sampling equation, e.g. z_{i_m} instead of i_m ?
- [X] remove old effects from table
- [X] mention CRP at the end of Section 2
- [X] Describe sampling of mu and sigma 
- [X] Fix description of intensities to have new ego setup
  events... shrinkage towards upper level mu
- [X] Move split merge entirely to appendix
- [X] fix  description in paper for pshift ('ego' issue)
- [ ] Incorporate SNRG note
- [X] cite reality mining paper and mcfarland and other data sets
- [ ] mention why we can't use EM
- [ ] rotate plots for synthetic example
- [X] more convincing motivation in Section 1
- [X] ditch handcock/schweinberger reference?
- [X] change title
- [X] mention continuous-time Markov jump process in Model section
- [X] Add mention of the supplemental material.
- [X] Add NIPSy references: Xing, Teh.
- [ ] Add little bits of information/details.  e.g. Interestingly...
- [X] Remove description of model specification with rich-get-richer bit
- [X] Mention what happens with cluster pairs that have no data
- [X] Mention sensitivity of hyperparameters to overfitting
* Bugs
- [X] TODO NANs created by llk_node in gibbs().  Only happening on Twitter occasionally.
- [X] TODO Huge hit in loglikelihood as soon as a new cluster appears?
- [X] add statistic transformation as an option in brem()
- [X] Small sigma hyperparameters crashing things. (From extreme
  normals that get generated after large sigma is generated.)
* Smyth meeting
- [X] Look at predictions from *last* sample
- [X] Add option for hard upper limit on number of clusters
- [ ] Consider other hyperparameter settings
