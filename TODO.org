* TODO Week 1:
- [X] Need to change block.ps to instead look at dyad-specific p-shifts.
[done?] Use a visualization of these p-shifts to explore the properities of the synthetic data.
- [X] Fix llk, writeup to account for sender/receiver for both i and j
- [X] Fix pshift to account for self link issue
- [X] Clean up write up and send to Padhraic.  
- [X] Get sense of size of dataset we can use.
- [X] Write mle for dyad-indpenent REM model.  (Double check llk function.)
- [X] Write sampler for blockmodel.
- [X] Fit blockmodel to synthetic data.

* TODO Week 2: 
- [X] Write SBM code.
- [X] Fit SBM to subset of Eckmann (dyadic only)
- [X] Fit REM separately to each block.
- [X] Implement BREM for submodels by modifying MH call in MCMC.
- [X] Setup prediction llk experiment and ranking code. Handle case where we have draws from posterior.
- [X] Setup experiment comparing submodels on synthetic data: k=1 and just intercept, k=1, beta for diag and gamma for off diag, and full model.
[done. not successful.] Noncollapsing clusters: try initializing beta centered on the mean overall rate.
- [X] Download some twitter data
- [X] Explore performance of approximate likelihood. [Needs roughly N/2 to be good in simple examples.]
Debug issues with whether to include intercept term or not. (Understand collinearity/identifiability issue.)
- [X] Reimplement brem.llk to take account of affected (i,j) that aren't in block (a,b).
- [X] Reimplement brem code with Arrays
- [X] Understand why we do better than truth on llk/ranking. [It was a bug in llk.]
[done, still buggy] Make faster llk function that doesn't use lrm w/ llk.
[done. overhead too big?] Try to speed up with OpenMP
- [X] Implement gibbs sampler for latent group assignment.
- Pass tests with one that computes mp ahead of time.
- Determine what's slowing things down.  Overhead?

* TODO Week 3:
- [X] Add features for saving progress during MCMC
- [X] Clean up experiment code so that you can fit all needed models on arbitrary dataset
- [X] Pull out code for plotting results from experiment
- [X] Compare submodels for eckmann subset with K=2. [online baseline was much better than BREM.]
- [X] Include degree effects (transform: sqrt? log?)
- [X] Get test suite to display results on all test scripts.
- [X] Make into a package.
* TODO Week 4:
- Try eckmann-small and synthetic dataset with degree effects.  
  - [x] Tinker with simulating with degree effects.  Perhaps come up with
    other parameterization that is less sensitive to explosion.
- Work on better proposal dist. and better mixing of group assignments.
  - Try split moves where we select a subset and offer them a new
    cluster with the same parameters as the previous cluster.  Try an
    experiment with this method where we have 2 true clusters and
    initialize everyone in one cluster.
  - Or just use tons of chains with few iterations?
- Understand why we outperform true model in synthetic example
- Add tests for Gibbs equations.  For example, start with true data
  and learn z's.
- Add tests for synthetic fitting?
- Derive/check identifiability
- [X] Type up Gibbs sampling equations.
- Include random effect options.  Compute_lambda just needs to know
  where to look in beta vector.  Proposal needs to not suggest that
  there are K^2 of these parameters, instead just one set shared
  across the various latent groups.
- Log likelihood baseline that only knows timing effects.
- Write abstract.
- Add Enron.
- Add UC Irvine dataset
- Add px feature to define which parameters to fit.
- [X] See if we can learn groups on synthetic data.
- Derive/implement EM for baseline model.
- Write up other possibilities for layering additional types of data, DP, or other extensions.
- Try random groups and see if REM still picks out (significantly) different parameters
- Consider MIT dataset or classroom dataset.
- Clean up interfaces to Rcpp code.  
  - Put all constants in RemStat.  Take out of ComputeLambdaFast, etc.
- Document Rcpp code better.
- [X] Add other statistics other than pshifts
  - Implement shared partners.
- Initialize with guess at z's, rather than guess at beta.
- Research maximization of beta rather than sampling.

* Low priority:
- Port simulator over to Rcpp.

* Bugs:
- old likelihood does not use the baserates when computing first event for the likelihood computation
